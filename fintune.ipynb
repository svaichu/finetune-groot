{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ca2843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/gr00t/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gr00t\n",
    "from gr00t.policy.gr00t_policy import Gr00tPolicy\n",
    "from custom_modality_config import *\n",
    "from gr00t.data.types import EmbodimentTag\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d851a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f583c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9267286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register_modality_config(my_config, embodiment_tag=EmbodimentTag.NEW_EMBODIMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193e7753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a174f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/workspace/gr00t/.venv/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune backbone llm: False\n",
      "Tune backbone visual: False\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.post_attention_layernorm.weight\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.post_attention_layernorm.weight to fp32\n",
      "Total number of DiT parameters:  1091722240\n",
      "Using AlternateVLDiT for diffusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/gr00t/gr00t/model/modules/dit.py:205: FutureWarning: Accessing config attribute `compute_dtype` directly via 'AlternateVLDiT' object attribute is deprecated. Please access 'compute_dtype' over 'AlternateVLDiT's config object instead, e.g. 'unet.config.compute_dtype'.\n",
      "  embedding_dim=self.inner_dim, compute_dtype=self.compute_dtype\n",
      "/workspace/gr00t/gr00t/model/modules/dit.py:236: FutureWarning: Accessing config attribute `output_dim` directly via 'AlternateVLDiT' object attribute is deprecated. Please access 'output_dim' over 'AlternateVLDiT's config object instead, e.g. 'unet.config.output_dim'.\n",
      "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n",
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n",
      "Tune action head vlln: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.04it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'new_embodiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mGr00tPolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membodiment_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEmbodimentTag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_EMBODIMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnvidia/GR00T-N1.6-3B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, modality_configs=my_config)\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/gr00t/gr00t/policy/gr00t_policy.py:93\u001b[0m, in \u001b[0;36mGr00tPolicy.__init__\u001b[0;34m(self, embodiment_tag, model_path, device, strict)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Store embodiment-specific configurations\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membodiment_tag \u001b[38;5;241m=\u001b[39m embodiment_tag\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_modality_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membodiment_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mcollator\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Extract and validate language configuration\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Currently only supports single language input per timestep\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new_embodiment'"
     ]
    }
   ],
   "source": [
    "policy = Gr00tPolicy(embodiment_tag=EmbodimentTag.NEW_EMBODIMENT, model_path=\"nvidia/GR00T-N1.6-3B\", device=device)#, modality_configs=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1a9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.model.gr00t_n1d6.gr00t_n1d6 import Gr00tN1d6, Gr00tN1d6Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8996d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Gr00tN1d6Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7df961",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Siglip2Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGr00tN1d6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/gr00t/gr00t/model/gr00t_n1d6/gr00t_n1d6.py:441\u001b[0m, in \u001b[0;36mGr00tN1d6.__init__\u001b[0;34m(self, config, transformers_loading_kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    440\u001b[0m backbone_cls \u001b[38;5;241m=\u001b[39m get_backbone_cls(config)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune_visual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_visual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselect_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreproject_vision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreproject_vision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_bf16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_bf16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune_top_llm_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_top_llm_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable_params_fp32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone_trainable_params_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_loading_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_loading_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Initialize action head\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_head \u001b[38;5;241m=\u001b[39m Gr00tN1d6ActionHead(config)\n",
      "File \u001b[0;32m/workspace/gr00t/gr00t/model/modules/eagle_backbone.py:47\u001b[0m, in \u001b[0;36mEagleBackbone.__init__\u001b[0;34m(self, model_name, tune_llm, tune_visual, select_layer, reproject_vision, use_flash_attention, projector_dim, load_bf16, tune_top_llm_layers, trainable_params_fp32, transformers_loading_kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     eagle_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEagle-Block2A-2B-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(eagle_path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:437\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    436\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    439\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2032\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2032\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;66;03m# restore default dtype if it was modified\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Eagle-Block2A-2B-v2/modeling_eagle3_vl.py:100\u001b[0m, in \u001b[0;36mEagle3_VLForConditionalGeneration.__init__\u001b[0;34m(self, config, vision_model, language_model)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mvision_config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiglip2_vision_model\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     99\u001b[0m     config\u001b[38;5;241m.\u001b[39mvision_config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m \u001b[43mSiglip2VisionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mvision_config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mradio\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m RADIOModel(config\u001b[38;5;241m.\u001b[39mvision_config)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Eagle-Block2A-2B-v2/modeling_siglip2.py:1374\u001b[0m, in \u001b[0;36mSiglip2VisionModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m Siglip2VisionTransformer(config)\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1897\u001b[0m, in \u001b[0;36mPreTrainedModel.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost_init\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;124;03m    A method executed at the end of each Transformer model initialization, to execute code that needs the model's\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;124;03m    modules properly initialized (such as weight initialization).\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1897\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_compatibility_gradient_checkpointing()\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;66;03m# Make sure the modules correctly exist if the flag is active\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3079\u001b[0m, in \u001b[0;36mPreTrainedModel.init_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads)\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_weights:\n\u001b[1;32m   3078\u001b[0m     \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[0;32m-> 3079\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3081\u001b[0m     \u001b[38;5;66;03m# Tie weights should be skipped when not initializing all weights\u001b[39;00m\n\u001b[1;32m   3082\u001b[0m     \u001b[38;5;66;03m# since from_pretrained(...) calls tie weights anyways\u001b[39;00m\n\u001b[1;32m   3083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1044\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1044\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1044\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1044\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1044\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1044\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1045\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1044\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m-> 1045\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspace/gr00t/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2451\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2452\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Eagle-Block2A-2B-v2/modeling_siglip2.py:1311\u001b[0m, in \u001b[0;36mSiglip2PreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1309\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_uniform_(module\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_weight\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1310\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(module\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_bias\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m-> 1311\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, \u001b[43mSiglip2Model\u001b[49m):\n\u001b[1;32m   1312\u001b[0m     logit_scale_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m))\n\u001b[1;32m   1313\u001b[0m     module\u001b[38;5;241m.\u001b[39mlogit_scale\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(logit_scale_init)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Siglip2Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = Gr00tN1d6(config=model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59db3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.configs.finetune_config import FinetuneConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ee0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FinetuneConfig(\n",
    "    base_model_path=\"nvidia/GR00T-N1.6-3B\",\n",
    "    dataset_path=\"/root/.cache/huggingface/lerobot/LSY-lab/stack_without_ft_tact_v4\",\n",
    "    embodiment_tag=EmbodimentTag.NEW_EMBODIMENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67151f89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FinetuneConfig' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FinetuneConfig' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c66f7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.model import MODEL_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a62d53ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{gr00t.configs.model.gr00t_n1d6.Gr00tN1d6Config: gr00t.model.gr00t_n1d6.setup.Gr00tN1d6Pipeline}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d9a1b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FinetuneConfig' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m MODEL_REGISTRY\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtype\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m))(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FinetuneConfig' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "pipeline = MODEL_REGISTRY.get(type(config.model))(config, \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "673529c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.experiment.launch_finetune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7b720ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08b380fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gr00tN1d6Config(model_type='Gr00tN1d6', model_dtype='bfloat16', model_name='nvidia/Eagle-Block2A-2B-v2', backbone_model_type='eagle', model_revision=None, tune_top_llm_layers=4, backbone_embedding_dim=2048, tune_llm=False, tune_visual=False, select_layer=16, reproject_vision=False, use_flash_attention=True, load_bf16=True, collator_overwrite_image_inputs=False, eagle_collator=False, backbone_trainable_params_fp32=True, image_crop_size=None, image_target_size=None, shortest_image_edge=256, crop_fraction=0.95, random_rotation_angle=None, color_jitter_params=None, use_albumentations_transforms=True, formalize_language=True, apply_sincos_state_encoding=False, use_relative_action=False, max_state_dim=29, max_action_dim=29, action_horizon=16, hidden_size=1024, input_embedding_dim=1536, add_pos_embed=True, attn_dropout=0.2, use_vlln=True, max_seq_len=1024, use_alternate_vl_dit=True, attend_text_every_n_blocks=2, diffusion_model_cfg={'positional_embeddings': None, 'num_layers': 32, 'num_attention_heads': 32, 'attention_head_dim': 48, 'norm_type': 'ada_norm', 'dropout': 0.2, 'final_dropout': True, 'output_dim': 1024, 'interleave_self_attention': True}, num_inference_timesteps=4, noise_beta_alpha=1.5, noise_beta_beta=1.0, noise_s=0.999, num_timestep_buckets=1000, tune_projector=True, tune_diffusion_model=True, tune_vlln=True, state_dropout_prob=0.0, state_additive_noise_scale=0.0, max_num_embodiments=32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
