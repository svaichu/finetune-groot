{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093891a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bda27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.configs.data.embodiment_configs import register_modality_config\n",
    "from gr00t.data.types import ModalityConfig, ActionConfig, ActionType, ActionRepresentation, ActionFormat\n",
    "from gr00t.data.types import EmbodimentTag\n",
    "\n",
    "\n",
    "my_config = {\n",
    "    \"video\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"observation.images.primary\",\n",
    "            \"observation.images.wrist\",\n",
    "        ],\n",
    "    ),\n",
    "    \"state\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"observation.state.cartesian\",\n",
    "            \"observation.state.gripper\",\n",
    "            \"observation.state.joints\",\n",
    "            \"observation.state.target\"\n",
    "        ],\n",
    "        action_configs=[\n",
    "            # cartesian\n",
    "            ActionConfig(\n",
    "                rep=ActionRepresentation.RELATIVE, # TODO: lookup doc of dataset\n",
    "                type=ActionType.EEF,\n",
    "                format=ActionFormat.DEFAULT,\n",
    "            ),\n",
    "            # gripper\n",
    "            ActionConfig(\n",
    "                rep=ActionRepresentation.ABSOLUTE,\n",
    "                type=ActionType.NON_EEF,\n",
    "                format=ActionFormat.DEFAULT,\n",
    "            ),\n",
    "            # joints\n",
    "            ActionConfig(\n",
    "                rep=ActionRepresentation.ABSOLUTE,\n",
    "                type=ActionType.NON_EEF,\n",
    "                format=ActionFormat.DEFAULT,\n",
    "            ),\n",
    "            # target\n",
    "            ActionConfig(\n",
    "                rep=ActionRepresentation.ABSOLUTE, #TODO: lookup doc of dataset\n",
    "                type=ActionType.EEF,\n",
    "                format=ActionFormat.DEFAULT,\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"action\": ModalityConfig(\n",
    "        delta_indices=list(range(0, 7)),\n",
    "        modality_keys=[\n",
    "            \"action\"\n",
    "        ],\n",
    "        action_configs=[\n",
    "            # single_arm\n",
    "            ActionConfig(\n",
    "                rep=ActionRepresentation.ABSOLUTE,\n",
    "                type=ActionType.NON_EEF,\n",
    "                format=ActionFormat.DEFAULT,\n",
    "            ),\n",
    "            # gripper\n",
    "            # ActionConfig(\n",
    "            #     rep=ActionRepresentation.ABSOLUTE,\n",
    "            #     type=ActionType.NON_EEF,\n",
    "            #     format=ActionFormat.DEFAULT,\n",
    "            # ),\n",
    "        ],\n",
    "    ),\n",
    "    \"language\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"task\"],\n",
    "    ),\n",
    "}\n",
    "\n",
    "register_modality_config(my_config, embodiment_tag=EmbodimentTag.NEW_EMBODIMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16135f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/gr00t/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gr00t\n",
    "\n",
    "from gr00t.data.dataset.lerobot_episode_loader import LeRobotEpisodeLoader\n",
    "from gr00t.data.dataset.sharded_single_step_dataset import extract_step_data\n",
    "from gr00t.policy.gr00t_policy import Gr00tPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb776fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<EmbodimentTag.NEW_EMBODIMENT: 'new_embodiment'>: {'video': ModalityConfig(delta_indices=[0], modality_keys=['observation.images.primary', 'observation.images.wrist'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['observation.state.cartesian', 'observation.state.gripper', 'observation.state.joints', 'observation.state.target'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.EEF: 'eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.EEF: 'eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)]),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6], modality_keys=['action'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)]),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['task'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)},\n",
       " 'unitree_g1': {'video': ModalityConfig(delta_indices=[0], modality_keys=['ego_view'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['left_leg', 'right_leg', 'waist', 'left_arm', 'right_arm', 'left_hand', 'right_hand'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], modality_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist', 'base_height_command', 'navigate_command'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)]),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['annotation.human.task_description'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)},\n",
       " 'libero_panda': {'video': ModalityConfig(delta_indices=[0], modality_keys=['image', 'wrist_image'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['annotation.human.action.task_description'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)},\n",
       " 'oxe_widowx': {'video': ModalityConfig(delta_indices=[0], modality_keys=['image_0'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'pad', 'gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7], modality_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw'], action_configs=None),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['annotation.human.action.task_description'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)},\n",
       " 'oxe_google': {'video': ModalityConfig(delta_indices=[0], modality_keys=['image'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['x', 'y', 'z', 'rx', 'ry', 'rz', 'rw', 'gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7], modality_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=['x', 'y', 'z', 'roll', 'pitch', 'yaw'], action_configs=None),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['annotation.human.action.task_description'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)},\n",
       " 'behavior_r1_pro': {'video': ModalityConfig(delta_indices=[0], modality_keys=['observation.images.rgb.head_256_256', 'observation.images.rgb.left_wrist_256_256', 'observation.images.rgb.right_wrist_256_256'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['robot_pos', 'robot_ori_cos', 'robot_ori_sin', 'robot_2d_ori', 'robot_2d_ori_cos', 'robot_2d_ori_sin', 'robot_lin_vel', 'robot_ang_vel', 'arm_left_qpos', 'arm_left_qpos_sin', 'arm_left_qpos_cos', 'eef_left_pos', 'eef_left_quat', 'gripper_left_qpos', 'arm_right_qpos', 'arm_right_qpos_sin', 'arm_right_qpos_cos', 'eef_right_pos', 'eef_right_quat', 'gripper_right_qpos', 'trunk_qpos'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], modality_keys=['base', 'torso', 'left_arm', 'left_gripper', 'right_arm', 'right_gripper'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key='trunk_qpos'), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key='arm_left_qpos'), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key='arm_right_qpos'), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)]),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['annotation.human.coarse_action'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)},\n",
       " 'new_embodiment': {'video': ModalityConfig(delta_indices=[0], modality_keys=['observation.images.primary', 'observation.images.wrist'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None),\n",
       "  'state': ModalityConfig(delta_indices=[0], modality_keys=['observation.state.cartesian', 'observation.state.gripper', 'observation.state.joints', 'observation.state.target'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.EEF: 'eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.EEF: 'eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)]),\n",
       "  'action': ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6], modality_keys=['action'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)]),\n",
       "  'language': ModalityConfig(delta_indices=[0], modality_keys=['task'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gr00t.configs.data.embodiment_configs import *\n",
    "MODALITY_CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following paths\n",
    "MODEL_PATH = \"nvidia/GR00T-N1.6-3B\"\n",
    "\n",
    "# REPO_PATH is the path of the pip install gr00t repo and one level up\n",
    "# REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
    "# DATASET_PATH = os.path.join(REPO_PATH, \"demo_data/gr1.PickNPlace\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b31cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/workspace/gr00t/.venv/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import gr00t\n",
    "import gr00t.model\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"nvidia/GR00T-N1.6-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6239cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"nvidia/GR00T-N1.6-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gr00t.model.gr00t_n1d6.processing_gr00t_n1d6.Gr00tN1d6Processor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2adcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['behavior_r1_pro', 'gr1', 'robocasa_panda_omron'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.get_modality_configs().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ca0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"modality_configs\": [my_config]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909052a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune backbone llm: False\n",
      "Tune backbone visual: False\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.post_attention_layernorm.weight\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.post_attention_layernorm.weight to fp32\n",
      "Total number of DiT parameters:  1091722240\n",
      "Using AlternateVLDiT for diffusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/gr00t/gr00t/model/modules/dit.py:205: FutureWarning: Accessing config attribute `compute_dtype` directly via 'AlternateVLDiT' object attribute is deprecated. Please access 'compute_dtype' over 'AlternateVLDiT's config object instead, e.g. 'unet.config.compute_dtype'.\n",
      "  embedding_dim=self.inner_dim, compute_dtype=self.compute_dtype\n",
      "/workspace/gr00t/gr00t/model/modules/dit.py:236: FutureWarning: Accessing config attribute `output_dim` directly via 'AlternateVLDiT' object attribute is deprecated. Please access 'output_dim' over 'AlternateVLDiT's config object instead, e.g. 'unet.config.output_dim'.\n",
      "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n",
      "Tune action head vlln: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.95it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'new_embodiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the correct embodiment tag variable and ensure the tag exists in your model/config.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# If you are adding a new embodiment, make sure it is registered and supported by your model weights and config.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mGr00tPolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43membodiment_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEmbodimentTag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_EMBODIMENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(policy\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m/workspace/gr00t/gr00t/policy/gr00t_policy.py:93\u001b[0m, in \u001b[0;36mGr00tPolicy.__init__\u001b[0;34m(self, embodiment_tag, model_path, device, strict)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Store embodiment-specific configurations\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membodiment_tag \u001b[38;5;241m=\u001b[39m embodiment_tag\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_modality_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membodiment_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mcollator\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Extract and validate language configuration\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Currently only supports single language input per timestep\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new_embodiment'"
     ]
    }
   ],
   "source": [
    "# Use the correct embodiment tag variable and ensure the tag exists in your model/config.\n",
    "# If you are adding a new embodiment, make sure it is registered and supported by your model weights and config.\n",
    "\n",
    "policy = Gr00tPolicy(\n",
    "    model_path=MODEL_PATH,\n",
    "    embodiment_tag=EmbodimentTag.NEW_EMBODIMENT,\n",
    "    device=device,\n",
    "    strict=True,\n",
    ")\n",
    "print(\"Policy loaded successfully.\")\n",
    "print(policy.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
